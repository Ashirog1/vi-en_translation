{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/only_u/Desktop/tmp/vdt/vdt_project1/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "84780"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import math\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "len(spacy_en.vocab.strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'is_interative_notebook' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 10\u001b[0m\n\u001b[1;32m      6\u001b[0m         vocab_src, vocab_tgt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m\"\u001b[39m\u001b[39mvocab_envi.pt\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[39mreturn\u001b[39;00m vocab_src, vocab_tgt\n\u001b[0;32m---> 10\u001b[0m \u001b[39mif\u001b[39;00m is_interative_notebook():\n\u001b[1;32m     11\u001b[0m     spacy_en \u001b[39m=\u001b[39m show_example(load_tokenizers)\n\u001b[1;32m     12\u001b[0m     vocab_src, vocab_tgt \u001b[39m=\u001b[39m show_example(load_vocab, args\u001b[39m=\u001b[39m[spacy_en])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'is_interative_notebook' is not defined"
     ]
    }
   ],
   "source": [
    "def load_vocab(spacy_en):\n",
    "    if not exists(\"vocab_envi.pt\"):\n",
    "        vocab_src, vocab_tgt = build_vocabulary(spacy_en)\n",
    "        torch.save((vocab_src, vocab_tgt), \"vocab_envi.pt\")\n",
    "    else:\n",
    "        vocab_src, vocab_tgt = torch.load(\"vocab_envi.pt\")\n",
    "\n",
    "    return vocab_src, vocab_tgt\n",
    "\n",
    "if is_interative_notebook():\n",
    "    spacy_en = show_example(load_tokenizers)\n",
    "    vocab_src, vocab_tgt = show_example(load_vocab, args=[spacy_en])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import torch\n",
    "\n",
    "spacy_en = spacy.load('en_core_web_sm')\n",
    "\n",
    "def get_dependency_info(x):\n",
    "    # Convert the tensor to a string\n",
    "    word_str = spacy_en.vocab.strings[x.item()]\n",
    "\n",
    "    # Tokenize the string using spaCy\n",
    "    doc = spacy_en(word_str)\n",
    "\n",
    "    # Get the dependency parsing information for the word\n",
    "    dep_info = [(token.text, token.dep_, token.head.text) for token in doc if token.text == word_str]\n",
    "\n",
    "    return dep_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4779645629803372569])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '4779645629803372569'. This usually refers to an issue with the `Vocab` or `StringStore`.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([spacy_en\u001b[39m.\u001b[39mvocab\u001b[39m.\u001b[39mstrings[\u001b[39m'\u001b[39m\u001b[39mapple is red\u001b[39m\u001b[39m'\u001b[39m]])\n\u001b[1;32m      2\u001b[0m \u001b[39mprint\u001b[39m(x)\n\u001b[0;32m----> 3\u001b[0m dep_info \u001b[39m=\u001b[39m get_dependency_info(x)\n\u001b[1;32m      4\u001b[0m \u001b[39mprint\u001b[39m(dep_info)\n",
      "Cell \u001b[0;32mIn[4], line 8\u001b[0m, in \u001b[0;36mget_dependency_info\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_dependency_info\u001b[39m(x):\n\u001b[1;32m      7\u001b[0m     \u001b[39m# Convert the tensor to a string\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m     word_str \u001b[39m=\u001b[39m spacy_en\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mstrings[x\u001b[39m.\u001b[39;49mitem()]\n\u001b[1;32m     10\u001b[0m     \u001b[39m# Tokenize the string using spaCy\u001b[39;00m\n\u001b[1;32m     11\u001b[0m     doc \u001b[39m=\u001b[39m spacy_en(word_str)\n",
      "File \u001b[0;32m~/Desktop/tmp/vdt/vdt_project1/.venv/lib/python3.8/site-packages/spacy/strings.pyx:159\u001b[0m, in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '4779645629803372569'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
     ]
    }
   ],
   "source": [
    "x = torch.tensor([spacy_en.vocab.strings['apple is red']])\n",
    "print(x)\n",
    "dep_info = get_dependency_info(x)\n",
    "print(dep_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.dep_lut = nn.Embedding(len(spacy_en.vocab.strings), d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Convert the input tensor to a list of strings\n",
    "        x = [spacy_en.vocab.strings[token.item()] for token in x]\n",
    "\n",
    "        # Tokenize the input using spaCy\n",
    "        doc = spacy_en(' '.join(x))\n",
    "\n",
    "        # Get the word embeddings for the input tokens\n",
    "        word_embeddings = self.lut(x)\n",
    "\n",
    "        # Get the dependency embeddings for the input tokens\n",
    "        dep_info = [doc.vocab.strings[token.dep_] for token in doc]\n",
    "        dep_embeddings = self.dep_lut(torch.tensor(dep_info))\n",
    "\n",
    "        # Concatenate the word embeddings and the dependency embeddings\n",
    "        embeddings = torch.cat([word_embeddings, dep_embeddings], dim=1)\n",
    "\n",
    "        # Scale the embeddings by the square root of the embedding dimension\n",
    "        embeddings *= math.sqrt(self.d_model)\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/only_u/Desktop/tmp/vdt/vdt_project1/.venv/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import phonlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[54, 18, 31, 12, 20,  4, 19, 92,  5, 18],\n",
      "        [ 2, 39, 11, 68,  4, 74,  4, 33, 14, 19],\n",
      "        [53, 86, 74, 79, 19, 46, 15, 32, 11, 65]], dtype=torch.int32)\n",
      "['FLAG54', 'IS_CURRENCY', 'FLAG31', 'IS_STOP', 'FLAG20', 'IS_LOWER', 'FLAG19', 'NOUN', 'IS_PUNCT', 'IS_CURRENCY']\n",
      "FLAG54 nummod IS_CURRENCY PROPN []\n",
      "IS_CURRENCY ROOT IS_CURRENCY PROPN [FLAG54, FLAG31]\n",
      "FLAG31 appos IS_CURRENCY PROPN []\n",
      "IS_STOP ROOT IS_STOP NOUN []\n",
      "FLAG20 compound IS_LOWER NOUN []\n",
      "IS_LOWER ROOT IS_LOWER NOUN [FLAG20, FLAG19]\n",
      "FLAG19 punct IS_LOWER NOUN []\n",
      "NOUN nsubj IS_PUNCT VERB []\n",
      "IS_PUNCT ROOT IS_PUNCT VERB [NOUN, IS_CURRENCY]\n",
      "IS_CURRENCY dobj IS_PUNCT VERB []\n",
      "['IS_ASCII', 'FLAG39', 'LIKE_EMAIL', 'SHAPE', 'IS_LOWER', 'POS', 'IS_LOWER', 'FLAG33', 'IS_BRACKET', 'FLAG19']\n",
      "IS_ASCII ROOT IS_ASCII X [FLAG39, LIKE_EMAIL]\n",
      "FLAG39 dep IS_ASCII X []\n",
      "LIKE_EMAIL punct IS_ASCII X []\n",
      "SHAPE compound IS_LOWER PROPN []\n",
      "IS_LOWER compound POS PROPN [SHAPE]\n",
      "POS compound IS_LOWER PROPN [IS_LOWER]\n",
      "IS_LOWER compound FLAG33 NOUN [POS]\n",
      "FLAG33 ROOT FLAG33 NOUN [IS_LOWER]\n",
      "IS_BRACKET ROOT IS_BRACKET NOUN [FLAG19]\n",
      "FLAG19 punct IS_BRACKET NOUN []\n",
      "['FLAG53', 'ADV', 'POS', 'HEAD', 'FLAG19', 'FLAG46', 'IS_QUOTE', 'FLAG32', 'LIKE_EMAIL', 'ORTH']\n",
      "FLAG53 nummod HEAD NOUN []\n",
      "ADV compound POS PROPN []\n",
      "POS compound HEAD NOUN [ADV]\n",
      "HEAD ROOT HEAD NOUN [FLAG53, POS, FLAG19, FLAG46]\n",
      "FLAG19 advmod HEAD NOUN []\n",
      "FLAG46 punct HEAD NOUN []\n",
      "IS_QUOTE ROOT IS_QUOTE PROPN [FLAG32, LIKE_EMAIL]\n",
      "FLAG32 appos IS_QUOTE PROPN []\n",
      "LIKE_EMAIL punct IS_QUOTE PROPN []\n",
      "ORTH ROOT ORTH PROPN []\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a tensor with size [1, len]\n",
    "size = (3, 10)\n",
    "\n",
    "# Initialize a positive integer random tensor\n",
    "tensor = torch.randint(low=1, high=100, size=size, dtype=torch.int)\n",
    "\n",
    "# Print the tensor\n",
    "print(tensor)\n",
    "# Convert each element of the tensor to a word using spaCy\n",
    "for i in range(tensor.shape[0]):\n",
    "    words = []\n",
    "    for t in tensor[i]:\n",
    "        word = nlp.vocab.strings[t.item()]\n",
    "        words.append(word)\n",
    "\n",
    "    print(words)\n",
    "\n",
    "    # Join the words into a string\n",
    "    text = \" \".join(words)\n",
    "\n",
    "    # Process the text with spaCy\n",
    "    doc = nlp(text)\n",
    "\n",
    "    # Print the dependency parse tree\n",
    "    for token in doc:\n",
    "        print(token.text, token.dep_, token.head.text, token.head.pos_,\n",
    "            [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"[E018] Can't retrieve string for hash '479'. This usually refers to an issue with the `Vocab` or `StringStore`.\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spacy_en\u001b[39m.\u001b[39;49mvocab\u001b[39m.\u001b[39;49mstrings[\u001b[39m479\u001b[39;49m]\n",
      "File \u001b[0;32m~/Desktop/tmp/vdt/vdt_project1/.venv/lib/python3.8/site-packages/spacy/strings.pyx:159\u001b[0m, in \u001b[0;36mspacy.strings.StringStore.__getitem__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"[E018] Can't retrieve string for hash '479'. This usually refers to an issue with the `Vocab` or `StringStore`.\""
     ]
    }
   ],
   "source": [
    "spacy_en.vocab.strings.StringStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "        for i in range(x.shape[0]):\n",
    "            words = []\n",
    "            for t in x[i]:\n",
    "                word = spacy_en.vocab.strings[t.item()]\n",
    "                words.append(word)\n",
    "            text = \" \".join(words)\n",
    "            doc = spacy_en(text)\n",
    "            dep_info = []\n",
    "            \n",
    "            for token in doc:\n",
    "                dep_info.append(token.dep_)\n",
    "            \n",
    "            dep_tensor[i] = torch.tensor([spacy_en.vocab.strings[s] for s in dep_info])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define a tensor with size [batch_size, channels, height, width]\n",
    "batch_size = 32\n",
    "channels = 3\n",
    "height = 224\n",
    "width = 224\n",
    "tensor = torch.randn(batch_size, channels, height, width)\n",
    "\n",
    "# Get the shape of the tensor\n",
    "shape = tensor.size()\n",
    "print(shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "an integer is required (got type str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m dep_tensors \u001b[39m=\u001b[39m []\n\u001b[1;32m     15\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m docs:\n\u001b[0;32m---> 16\u001b[0m     dep_tensor \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mtensor([[token\u001b[39m.\u001b[39;49mhead\u001b[39m.\u001b[39;49mi, token\u001b[39m.\u001b[39;49mi, token\u001b[39m.\u001b[39;49mdep_] \u001b[39mfor\u001b[39;49;00m token \u001b[39min\u001b[39;49;00m doc], dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mint)\n\u001b[1;32m     17\u001b[0m     dep_tensors\u001b[39m.\u001b[39mappend(dep_tensor)\n\u001b[1;32m     19\u001b[0m \u001b[39m# Print the dependency parse tensors\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: an integer is required (got type str)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import torch\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a list of sentences to parse\n",
    "sentences = [\"This is the first sentence.\", \"And this is the second one.\"]\n",
    "\n",
    "# Process the sentences with spaCy\n",
    "docs = list(nlp.pipe(sentences))\n",
    "\n",
    "# Extract the dependency parse tensors from the docs\n",
    "dep_tensors = []\n",
    "for doc in docs:\n",
    "    dep_tensor = torch.tensor([[token.head.i, token.i, token.dep_] for token in doc], dtype=torch.int)\n",
    "    dep_tensors.append(dep_tensor)\n",
    "\n",
    "# Print the dependency parse tensors\n",
    "for dep_tensor in dep_tensors:\n",
    "    print(dep_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(3, 0, 'det'), (3, 1, 'amod'), (3, 2, 'amod'), (4, 3, 'nsubj'), (4, 4, 'ROOT'), (4, 5, 'prep'), (8, 6, 'det'), (8, 7, 'amod'), (5, 8, 'pobj'), (4, 9, 'punct')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Define a sentence to parse\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Process the sentence with spaCy\n",
    "doc = nlp(sentence)\n",
    "\n",
    "# Extract the dependency information\n",
    "dep_info = []\n",
    "for token in doc:\n",
    "    # Append a tuple of (head index, token index, dependency label) to the list\n",
    "    dep_info.append((token.head.i, token.i, token.dep_))\n",
    "\n",
    "# Print the dependency information\n",
    "print(dep_info)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
